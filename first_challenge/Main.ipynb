{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jMqOF9jVc5S6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calde97/Deep_Learning_Challenge/blob/main/first_challenge/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp1OZCbCc9F4",
        "outputId": "6f08361e-95a6-4005-a07b-425deca79dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SEED = 1234 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMqOF9jVc5S6"
      },
      "source": [
        "# Download data from kaggle to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkoKbLHKdjOV"
      },
      "source": [
        "#I forced the upgrade since I had some problems with the previous API. To apply just once\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4L2LCbOhTiu"
      },
      "source": [
        "**From kaggle to colab** <br>\n",
        "We need to use the API from kaggle to allow the download from colab. We donwload the token as json and then save it into a hidden folder kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__qTb7O_eHJx"
      },
      "source": [
        "#Upload json kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU2B28KuemMZ",
        "outputId": "2ba02e4e-3427-4bfd-faf3-80a9d0d6918e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dz6p8Nxet-n"
      },
      "source": [
        " ! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0-MHZ0gGZF"
      },
      "source": [
        "!kaggle datasets list  #just curiosity for all the ds available on kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGKx73oPiH5G"
      },
      "source": [
        "Donwload Zip from kaggle competion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzCL0otzh9K5",
        "outputId": "88f0bcfe-a2c4-4600-8302-369dd92e2c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle competitions download -c artificial-neural-networks-and-deep-learning-2020\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading artificial-neural-networks-and-deep-learning-2020.zip to /content\n",
            " 96% 398M/413M [00:03<00:00, 176MB/s]\n",
            "100% 413M/413M [00:03<00:00, 140MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ6_TQWECN3W"
      },
      "source": [
        "Create directory challenge1 to unzip our ds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2NQUPWlrqtD"
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/challenge1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jEo9FKvkL9D"
      },
      "source": [
        "!unzip /content/artificial-neural-networks-and-deep-learning-2020.zip -d /content/drive/My\\ Drive/challenge1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV8izcOHC1xu"
      },
      "source": [
        "# Preprocess the data for tensorflow flow_from_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rONi0H_SDNFg"
      },
      "source": [
        "We now try to get the data clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfiyeSI5WlTY"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/challenge1/MaskDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwvFEZ92W0uk"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "with open('train_gt.json') as file:\n",
        "  data = json.load(file)\n",
        "items = data.items()\n",
        "image_target_list = list(items)\n",
        "targets = np.array(image_target_list)[:, 1]\n",
        "targets = np.unique(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLr4DuQ-5Jeg"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/challenge1/MaskDataset/training/2\n",
        "!ls | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVNbEtcaYe6Q"
      },
      "source": [
        "import os \n",
        "import shutil\n",
        "cwd = os.getcwd()\n",
        "training_path = os.path.join(cwd, 'training')\n",
        "\n",
        "for label in targets:\n",
        "  if not os.path.exists(os.path.join(training_path, str(label))):\n",
        "    os.mkdir(os.path.join(training_path, str(label)))\n",
        "\n",
        "for image, label in image_target_list:\n",
        "  shutil.move(os.path.join(training_path, image), os.path.join(training_path, str(label), image))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv5FD_AoetDw"
      },
      "source": [
        "# Building the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSf2n6eZq1WW"
      },
      "source": [
        "## Image Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPcZYraMHFlN"
      },
      "source": [
        "Image Augmentation (Zoom, brightness, small rootation, horizontal flip) the normalization. Since I don't want to create 2 separate folders I'm building 2 imagedatagenerator that slows from the same directory. I set the validation split equal to both. Also the SEED must be equal to take the same pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGaSeJlYhwsp",
        "outputId": "e3b55888-f5af-47ee-d357-5ab3d81bf2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os \n",
        "\n",
        "%cd /content/drive/My Drive/challenge1/MaskDataset\n",
        "cwd = os.getcwd()\n",
        "training_path = os.path.join(cwd, 'training')\n",
        "\n",
        "\n",
        "VALIDATION_SPLIT = 0.15\n",
        "bs = 8\n",
        "\n",
        "#Creating Training generator with augmentation\n",
        "train_image_data_generator = ImageDataGenerator(rescale=1/255.,\n",
        "                                                horizontal_flip=True,\n",
        "                                                rotation_range=30,\n",
        "                                                brightness_range=[0.2, 1.2],\n",
        "                                                zoom_range=[0.7, 1.3],\n",
        "                                                validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "train_data_generator = train_image_data_generator.flow_from_directory(training_path,\n",
        "                                                                      batch_size=bs,\n",
        "                                                                      seed=SEED,\n",
        "                                                                      subset='training',\n",
        "                                                                      class_mode='categorical')\n",
        "\n",
        "\n",
        "#Creating Validation generator\n",
        "validation_image_data_generator = ImageDataGenerator(rescale=1/255.,\n",
        "                                                     validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "\n",
        "validation_data_generator = validation_image_data_generator.flow_from_directory(training_path,\n",
        "                                                                                batch_size=bs,\n",
        "                                                                                seed=SEED,\n",
        "                                                                                subset='validation',\n",
        "                                                                                class_mode='categorical')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/challenge1/MaskDataset\n",
            "Found 841 images belonging to 3 classes.\n",
            "Found 841 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpPG4gAIHmll"
      },
      "source": [
        "image_width = 256\n",
        "image_height = 256\n",
        "channels = 3\n",
        "classes_number = 3\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda : train_data_generator,  ##There was a mistake here\n",
        "                                               output_types=(tf.float32, tf.float16),\n",
        "                                               output_shapes=([None, image_width, image_height, channels], [None, classes_number]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_generator(lambda : validation_data_generator,\n",
        "                                            output_types=(tf.float32, tf.float16),\n",
        "                                            output_shapes=([None, image_width, image_height, channels], [None, classes_number]))\n",
        "validation_dataset = validation_dataset.repeat()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaIKteQwHfuG"
      },
      "source": [
        "## Building the baseline MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYQf_sBxRNsc"
      },
      "source": [
        "Trying to use the model created in the 3rd lab w/o augmentation.\n",
        "###Feature extraction CNN + MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WoFcqszRKAS"
      },
      "source": [
        "start_f = 8\n",
        "depth = 5 \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "for i in range(depth):\n",
        "  if i == 0:        \n",
        "    input_shape = [image_width, image_height, channels]\n",
        "  else:\n",
        "    input_shape = [None]\n",
        "  \n",
        "  model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   padding='same',\n",
        "                                   strides=(1,1),\n",
        "                                   input_shape=input_shape))\n",
        "  \n",
        "  model.add(tf.keras.layers.ReLU())\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  start_f *= 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEHnFuQ6T16s"
      },
      "source": [
        "#Building the classifier (MLP)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=512,\n",
        "                                activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=classes_number, activation='softmax'))\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Q7u6xyf0Ci"
      },
      "source": [
        "### Model fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0jSIefgGBv"
      },
      "source": [
        "#Work in progess ahah"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJubHZhGGaw"
      },
      "source": [
        "# Create results file for test images (Not completed... waiting for the model to try )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thA5IUrX9MHQ"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnRy8OqrB4mn"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "results = {}\n",
        "\n",
        "test_path = os.path.join(cwd, 'test')\n",
        "names = os.listdir(test_path)\n",
        "\n",
        "\n",
        "for img_name in names:\n",
        "  img = Image.open(os.path.join(test_path, img_name)).convert('RGB')\n",
        "  nimg = np.array(img).shape\n",
        "  nimg = np.expand_dims(nimg, 0)\n",
        "  nimg = nimg / 255.\n",
        "  softmax_output = model.predict(nimg)\n",
        "  prediction = tf.argmax(softmax_output, 1)\n",
        "  class_label = prediction.numpy()[0]\n",
        "  results[img_name] = class_label\n",
        "\n",
        "create_csv(results,results_dir=\"/content/drive/My Drive/challenge1/MaskDataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2wb9WJDy3b9"
      },
      "source": [
        "# Not Useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuZgsWnQs_IN"
      },
      "source": [
        "#Not useful for now \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "def plot_image(augmented_img):\n",
        "  augmented_img = np.array(augmented_img)\n",
        "  augmented_img = augmented_img * 255\n",
        "  plt.imshow(np.uint8(augmented_img)) \n",
        "  plt.plot()\n",
        "\n",
        "#img, tg = train_data_generator.next()\n",
        "#plot_image(img[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}